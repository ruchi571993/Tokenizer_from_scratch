# Tokenizer_from_scratch
What if there is no pre-trained model that aligns with our specific requirements?  Maybe we’d like our model to understand a less common language for example, how many transformer models out there have been trained on Piemontese or the Nahuatl languages?  *zero*  In that case, we need to do something different. We need to build our own model — from scratch.
